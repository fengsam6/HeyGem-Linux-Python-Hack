#!/usr/bin/env python
# coding=utf-8
"""
GPUä¿®å¤æµ‹è¯•è„šæœ¬
éªŒè¯Gunicorn + GPUç¯å¢ƒæ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import requests
import json
import time
import subprocess
import sys
import os

def test_gpu_status(base_url="http://localhost:8383"):
    """æµ‹è¯•GPUçŠ¶æ€æ¥å£"""
    print("ğŸ” æµ‹è¯•GPUçŠ¶æ€æ¥å£...")
    try:
        response = requests.get(f"{base_url}/gpu/status", timeout=30)
        if response.status_code == 200:
            data = response.json()
            print("âœ… GPUçŠ¶æ€æ¥å£å“åº”æ­£å¸¸")
            
            # è§£æå“åº”
            if data.get('success'):
                gpu_info = data.get('data', {})
                print(f"ğŸ“Š Worker PID: {gpu_info.get('worker_pid')}")
                print(f"ğŸ¤– æ¨¡å‹å·²åˆå§‹åŒ–: {gpu_info.get('models_initialized')}")
                print(f"ğŸ”§ CUDAè®¾å¤‡: {gpu_info.get('cuda_visible_devices')}")
                
                # æ£€æŸ¥ONNX Runtime
                if gpu_info.get('onnx_cuda_available'):
                    print("âœ… ONNX Runtime CUDA: å¯ç”¨")
                else:
                    print("âŒ ONNX Runtime CUDA: ä¸å¯ç”¨")
                    print(f"   å¯ç”¨providers: {gpu_info.get('onnx_providers', [])}")
                
                # æ£€æŸ¥PyTorch
                if gpu_info.get('pytorch_cuda_available'):
                    print("âœ… PyTorch CUDA: å¯ç”¨")
                    print(f"   GPUæ•°é‡: {gpu_info.get('pytorch_gpu_count')}")
                    print(f"   GPUåç§°: {gpu_info.get('pytorch_gpu_name')}")
                    memory = gpu_info.get('pytorch_memory', {})
                    print(f"   æ˜¾å­˜ä½¿ç”¨: {memory.get('allocated_gb', 0):.2f}GB")
                else:
                    print("âŒ PyTorch CUDA: ä¸å¯ç”¨")
                
                return gpu_info.get('onnx_cuda_available', False)
            else:
                print(f"âŒ GPUçŠ¶æ€æ£€æŸ¥å¤±è´¥: {data.get('msg')}")
                return False
        else:
            print(f"âŒ HTTPè¯·æ±‚å¤±è´¥: {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ æµ‹è¯•GPUçŠ¶æ€æ—¶å‡ºé”™: {e}")
        return False

def test_health_check(base_url="http://localhost:8383"):
    """æµ‹è¯•å¥åº·æ£€æŸ¥æ¥å£"""
    print("\nğŸ’Š æµ‹è¯•å¥åº·æ£€æŸ¥æ¥å£...")
    try:
        response = requests.get(f"{base_url}/health", timeout=30)
        if response.status_code == 200:
            data = response.json()
            if data.get('success'):
                health_info = data.get('data', {})
                print("âœ… æœåŠ¡å¥åº·æ£€æŸ¥é€šè¿‡")
                print(f"ğŸ“Š Worker PID: {health_info.get('worker_pid')}")
                print(f"ğŸ¤– æ¨¡å‹å·²åˆå§‹åŒ–: {health_info.get('models_initialized')}")
                print(f"ğŸ“‹ é˜Ÿåˆ—é•¿åº¦: {health_info.get('queue_size')}")
                print(f"ğŸƒ å½“å‰ä»»åŠ¡æ•°: {health_info.get('current_tasks')}")
                return True
            else:
                print(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {data.get('msg')}")
                return False
        else:
            print(f"âŒ HTTPè¯·æ±‚å¤±è´¥: {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ å¥åº·æ£€æŸ¥æ—¶å‡ºé”™: {e}")
        return False

def wait_for_service(base_url="http://localhost:8383", max_wait=60):
    """ç­‰å¾…æœåŠ¡å¯åŠ¨"""
    print(f"â³ ç­‰å¾…æœåŠ¡å¯åŠ¨ (æœ€å¤š{max_wait}ç§’)...")
    
    for i in range(max_wait):
        try:
            response = requests.get(f"{base_url}/health", timeout=5)
            if response.status_code == 200:
                print(f"âœ… æœåŠ¡å·²å¯åŠ¨ (ç­‰å¾…äº†{i+1}ç§’)")
                return True
        except:
            pass
        
        if i % 10 == 0 and i > 0:
            print(f"   ä»åœ¨ç­‰å¾…... ({i}ç§’)")
        time.sleep(1)
    
    print(f"âŒ æœåŠ¡åœ¨{max_wait}ç§’å†…æœªå¯åŠ¨")
    return False

def check_gunicorn_config():
    """æ£€æŸ¥Gunicorné…ç½®"""
    print("\nğŸ”§ æ£€æŸ¥Gunicorné…ç½®...")
    
    if not os.path.exists('gunicorn.conf.py'):
        print("âŒ gunicorn.conf.pyä¸å­˜åœ¨")
        return False
    
    with open('gunicorn.conf.py', 'r', encoding='utf-8') as f:
        content = f.read()
        
        checks = {
            'preload_app = False': 'âœ…' if 'preload_app = False' in content else 'âŒ',
            'workers = 1': 'âœ…' if ('workers = 1' in content or 'workers = int(os.environ.get(\'GUNICORN_WORKERS\', 1))' in content) else 'âš ï¸',
            'CUDAç¯å¢ƒå˜é‡': 'âœ…' if 'CUDA_VISIBLE_DEVICES' in content else 'âš ï¸',
            'post_forké’©å­': 'âœ…' if 'def post_fork' in content else 'âŒ',
        }
        
        print("é…ç½®æ£€æŸ¥ç»“æœ:")
        for check, status in checks.items():
            print(f"  {status} {check}")
        
        return all(status == 'âœ…' for status in checks.values())

def run_quick_test():
    """è¿è¡Œå¿«é€Ÿæµ‹è¯•"""
    print("="*60)
    print("HeyGem AI - GPUä¿®å¤éªŒè¯æµ‹è¯•")
    print("CUDAå¤šè¿›ç¨‹å…¼å®¹æ€§æ£€æŸ¥")
    print("="*60)
    
    # æ£€æŸ¥é…ç½®
    config_ok = check_gunicorn_config()
    if not config_ok:
        print("\nâš ï¸  é…ç½®æ£€æŸ¥å‘ç°é—®é¢˜ï¼Œå»ºè®®æ£€æŸ¥gunicorn.conf.py")
    
    # ç­‰å¾…æœåŠ¡
    if not wait_for_service():
        print("\nâŒ æ— æ³•è¿æ¥åˆ°æœåŠ¡ï¼Œè¯·ç¡®ä¿æœåŠ¡å·²å¯åŠ¨")
        print("\nğŸ“‹ å¯åŠ¨å»ºè®®:")
        print("1. ç®€åŒ–ç‰ˆå¯åŠ¨å™¨ï¼ˆæ¨èï¼‰: python start_simple_gpu_server.py")
        print("2. CUDAå…¼å®¹å¯åŠ¨å™¨: python start_cuda_fixed_server.py")
        print("3. GPUä¼˜åŒ–è„šæœ¬: ./start_gpu_server.sh")
        print("4. æ‰‹åŠ¨å¯åŠ¨: python app_production.py")
        print("5. ç›´æ¥Gunicorn: gunicorn --config gunicorn.conf.py app_server:app")
        return False
    
    # æµ‹è¯•GPUçŠ¶æ€
    gpu_ok = test_gpu_status()
    
    # æµ‹è¯•å¥åº·æ£€æŸ¥
    health_ok = test_health_check()
    
    print("\n" + "="*60)
    print("æµ‹è¯•ç»“æœæ€»ç»“:")
    print("="*60)
    
    if gpu_ok and health_ok:
        print("ğŸ‰ æ­å–œï¼GPUä¿®å¤æˆåŠŸ!")
        print("âœ… ONNX Runtime GPUå¯ç”¨")
        print("âœ… æ¨¡å‹å·²åœ¨workerè¿›ç¨‹ä¸­æ­£ç¡®åˆå§‹åŒ–")
        print("âœ… Gunicorn + GPUç¯å¢ƒæ­£å¸¸å·¥ä½œ")
        print("\nğŸš€ ç°åœ¨å¯ä»¥æ­£å¸¸ä½¿ç”¨GPUåŠ é€Ÿçš„AIæœåŠ¡äº†ï¼")
        return True
    else:
        print("âŒ GPUä¿®å¤éªŒè¯å¤±è´¥")
        if not gpu_ok:
            print("âŒ GPUä¸å¯ç”¨æˆ–æœªæ­£ç¡®åˆå§‹åŒ–")
        if not health_ok:
            print("âŒ æœåŠ¡å¥åº·æ£€æŸ¥å¤±è´¥")
        
        print("\nğŸ”§ æ•…éšœæ’æŸ¥å»ºè®®:")
        print("1. è¿è¡Œè¯¦ç»†è¯Šæ–­: python check_gpu_gunicorn.py")
        print("2. æ£€æŸ¥æœåŠ¡æ—¥å¿—ä¸­çš„post_forkä¿¡æ¯")
        print("3. ç¡®è®¤GPUç¯å¢ƒå˜é‡è®¾ç½®æ­£ç¡®")
        print("4. éªŒè¯CUDAé©±åŠ¨å’Œonnxruntime-gpuå®‰è£…")
        print("5. å¦‚æœé‡åˆ°CUDAå¤šè¿›ç¨‹é”™è¯¯ï¼Œä½¿ç”¨: python start_cuda_fixed_server.py")
        return False

def run_stress_test():
    """è¿è¡Œå‹åŠ›æµ‹è¯•ï¼ˆå¯é€‰ï¼‰"""
    print("\n" + "="*60)
    print("GPUå‹åŠ›æµ‹è¯• (å¯é€‰)")
    print("="*60)
    
    choice = input("æ˜¯å¦è¿è¡ŒGPUå‹åŠ›æµ‹è¯•? (y/N): ").lower()
    if choice != 'y':
        print("è·³è¿‡å‹åŠ›æµ‹è¯•")
        return
    
    print("ğŸ”¥ å¼€å§‹GPUå‹åŠ›æµ‹è¯•...")
    
    # å‘é€å¤šä¸ªå¹¶å‘è¯·æ±‚æµ‹è¯•GPUç¨³å®šæ€§
    for i in range(5):
        print(f"æµ‹è¯•è½®æ¬¡ {i+1}/5...")
        gpu_ok = test_gpu_status()
        if not gpu_ok:
            print(f"âŒ ç¬¬{i+1}è½®æµ‹è¯•å¤±è´¥")
            break
        time.sleep(2)
    else:
        print("âœ… GPUå‹åŠ›æµ‹è¯•é€šè¿‡ï¼")

if __name__ == "__main__":
    try:
        # è¿è¡Œä¸»æµ‹è¯•
        success = run_quick_test()
        
        if success:
            # å¯é€‰çš„å‹åŠ›æµ‹è¯•
            run_stress_test()
        
        # é€€å‡ºç 
        sys.exit(0 if success else 1)
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1) 