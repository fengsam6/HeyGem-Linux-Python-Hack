#!/usr/bin/env python
# coding=utf-8
"""
GPU + Gunicorn ç¯å¢ƒè¯Šæ–­è„šæœ¬
ç”¨äºæ£€æŸ¥å¤§æ¨¡å‹æœåŠ¡åœ¨Gunicornç¯å¢ƒä¸‹çš„GPUé…ç½®æ˜¯å¦æ­£ç¡®
"""

import os
import sys
import subprocess
import traceback

def print_section(title):
    """æ‰“å°åˆ†èŠ‚æ ‡é¢˜"""
    print(f"\n{'='*50}")
    print(f" {title}")
    print(f"{'='*50}")

def check_basic_environment():
    """æ£€æŸ¥åŸºç¡€ç¯å¢ƒ"""
    print_section("åŸºç¡€ç¯å¢ƒæ£€æŸ¥")
    
    # Pythonç‰ˆæœ¬
    print(f"âœ… Pythonç‰ˆæœ¬: {sys.version}")
    
    # ç¯å¢ƒå˜é‡
    cuda_visible = os.environ.get('CUDA_VISIBLE_DEVICES', 'æœªè®¾ç½®')
    nvidia_visible = os.environ.get('NVIDIA_VISIBLE_DEVICES', 'æœªè®¾ç½®')
    pytorch_conf = os.environ.get('PYTORCH_CUDA_ALLOC_CONF', 'æœªè®¾ç½®')
    
    print(f"ğŸ”§ CUDA_VISIBLE_DEVICES: {cuda_visible}")
    print(f"ğŸ”§ NVIDIA_VISIBLE_DEVICES: {nvidia_visible}")
    print(f"ğŸ”§ PYTORCH_CUDA_ALLOC_CONF: {pytorch_conf}")

def check_nvidia_driver():
    """æ£€æŸ¥NVIDIAé©±åŠ¨"""
    print_section("NVIDIAé©±åŠ¨æ£€æŸ¥")
    
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            print("âœ… NVIDIAé©±åŠ¨æ­£å¸¸")
            lines = result.stdout.split('\n')
            for line in lines:
                if 'Driver Version' in line:
                    print(f"ğŸ“Š {line.strip()}")
                    break
        else:
            print("âŒ nvidia-smiå‘½ä»¤å¤±è´¥")
            print(result.stderr)
    except FileNotFoundError:
        print("âŒ nvidia-smiå‘½ä»¤æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥NVIDIAé©±åŠ¨å®‰è£…")
    except Exception as e:
        print(f"âŒ æ£€æŸ¥NVIDIAé©±åŠ¨æ—¶å‡ºé”™: {e}")

def check_onnx_runtime():
    """æ£€æŸ¥ONNX Runtime GPUæ”¯æŒ"""
    print_section("ONNX Runtime GPUæ£€æŸ¥")
    
    try:
        import onnxruntime
        print(f"âœ… ONNX Runtimeç‰ˆæœ¬: {onnxruntime.__version__}")
        
        providers = onnxruntime.get_available_providers()
        print(f"ğŸ“‹ å¯ç”¨providers: {providers}")
        
        if 'CUDAExecutionProvider' in providers:
            print("âœ… ONNX Runtime CUDAæ”¯æŒ: å¯ç”¨")
            
            # å°è¯•åˆ›å»ºCUDA session
            try:
                session_options = onnxruntime.SessionOptions()
                session_options.log_severity_level = 3
                
                # ä½¿ç”¨ä¸€ä¸ªç®€å•çš„æ¨¡å‹æµ‹è¯•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                test_model_paths = [
                    "./face_detect_utils/resources/scrfd_500m_bnkps_shape640x640.onnx",
                    "./check_env/test.onnx"  # å¦‚æœæœ‰æµ‹è¯•æ¨¡å‹
                ]
                
                model_found = False
                for model_path in test_model_paths:
                    if os.path.exists(model_path):
                        print(f"ğŸ§ª æµ‹è¯•æ¨¡å‹: {model_path}")
                        providers_list = [("CUDAExecutionProvider", {"device_id": 0})]
                        session = onnxruntime.InferenceSession(
                            model_path, 
                            session_options, 
                            providers=providers_list
                        )
                        actual_providers = session.get_providers()
                        print(f"âœ… å®é™…ä½¿ç”¨çš„providers: {actual_providers}")
                        model_found = True
                        break
                
                if not model_found:
                    print("âš ï¸  æœªæ‰¾åˆ°æµ‹è¯•æ¨¡å‹ï¼Œæ— æ³•éªŒè¯CUDAåŠŸèƒ½")
                    
            except Exception as e:
                print(f"âŒ CUDA sessionåˆ›å»ºå¤±è´¥: {e}")
        else:
            print("âŒ ONNX Runtime CUDAæ”¯æŒ: ä¸å¯ç”¨")
            print("è¯·æ£€æŸ¥æ˜¯å¦å®‰è£…äº†onnxruntime-gpuåŒ…")
            
    except ImportError:
        print("âŒ ONNX Runtimeæœªå®‰è£…")
    except Exception as e:
        print(f"âŒ æ£€æŸ¥ONNX Runtimeæ—¶å‡ºé”™: {e}")

def check_pytorch():
    """æ£€æŸ¥PyTorch CUDAæ”¯æŒ"""
    print_section("PyTorch CUDAæ£€æŸ¥")
    
    try:
        import torch
        print(f"âœ… PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"âœ… CUDAç¼–è¯‘ç‰ˆæœ¬: {torch.version.cuda}")
        
        if torch.cuda.is_available():
            print("âœ… PyTorch CUDAæ”¯æŒ: å¯ç”¨")
            print(f"ğŸ“Š GPUæ•°é‡: {torch.cuda.device_count()}")
            
            for i in range(torch.cuda.device_count()):
                print(f"ğŸ“Š GPU[{i}]: {torch.cuda.get_device_name(i)}")
                props = torch.cuda.get_device_properties(i)
                print(f"   - æ˜¾å­˜: {props.total_memory / 1024**3:.1f}GB")
                print(f"   - è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
            
            # å½“å‰è®¾å¤‡
            current_device = torch.cuda.current_device()
            print(f"ğŸ“Š å½“å‰è®¾å¤‡: GPU[{current_device}]")
            
            # æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
            memory_allocated = torch.cuda.memory_allocated() / 1024**3
            memory_reserved = torch.cuda.memory_reserved() / 1024**3
            print(f"ğŸ“Š æ˜¾å­˜ä½¿ç”¨: {memory_allocated:.2f}GB / ä¿ç•™: {memory_reserved:.2f}GB")
            
        else:
            print("âŒ PyTorch CUDAæ”¯æŒ: ä¸å¯ç”¨")
            
    except ImportError:
        print("âš ï¸  PyTorchæœªå®‰è£…")
    except Exception as e:
        print(f"âŒ æ£€æŸ¥PyTorchæ—¶å‡ºé”™: {e}")

def check_gunicorn_config():
    """æ£€æŸ¥Gunicorné…ç½®"""
    print_section("Gunicorné…ç½®æ£€æŸ¥")
    
    try:
        # æ£€æŸ¥é…ç½®æ–‡ä»¶
        if os.path.exists('gunicorn.conf.py'):
            print("âœ… gunicorn.conf.pyå­˜åœ¨")
            
            # è¯»å–å…³é”®é…ç½®
            with open('gunicorn.conf.py', 'r', encoding='utf-8') as f:
                content = f.read()
                
                # æ£€æŸ¥å…³é”®è®¾ç½®
                if 'preload_app = False' in content:
                    print("âœ… preload_app = False (GPUåº”ç”¨å¿…é¡»)")
                elif 'preload_app = True' in content:
                    print("âŒ preload_app = True (GPUåº”ç”¨åº”è¯¥ä¸ºFalse)")
                else:
                    print("âš ï¸  æœªæ‰¾åˆ°preload_appè®¾ç½®")
                
                if 'workers = 1' in content or 'workers = int(os.environ.get(\'GUNICORN_WORKERS\', 1))' in content:
                    print("âœ… workers = 1 (æ¨èç”¨äºGPUåº”ç”¨)")
                else:
                    print("âš ï¸  workers > 1 å¯èƒ½å¯¼è‡´GPUæ˜¾å­˜ä¸è¶³")
                
                if 'CUDA_VISIBLE_DEVICES' in content:
                    print("âœ… åŒ…å«CUDAç¯å¢ƒå˜é‡è®¾ç½®")
                else:
                    print("âš ï¸  æœªæ‰¾åˆ°CUDAç¯å¢ƒå˜é‡è®¾ç½®")
                    
        else:
            print("âŒ gunicorn.conf.pyä¸å­˜åœ¨")
            
    except Exception as e:
        print(f"âŒ æ£€æŸ¥Gunicorné…ç½®æ—¶å‡ºé”™: {e}")

def check_service_dependencies():
    """æ£€æŸ¥æœåŠ¡ä¾èµ–"""
    print_section("æœåŠ¡ä¾èµ–æ£€æŸ¥")
    
    try:
        # æ£€æŸ¥å…³é”®æ¨¡å—
        modules_to_check = [
            'service.trans_dh_service',
            'cv2',
            'numpy', 
            'flask',
            'gunicorn'
        ]
        
        for module in modules_to_check:
            try:
                __import__(module)
                print(f"âœ… {module}: å¯ç”¨")
            except ImportError as e:
                print(f"âŒ {module}: ä¸å¯ç”¨ - {e}")
                
    except Exception as e:
        print(f"âŒ æ£€æŸ¥ä¾èµ–æ—¶å‡ºé”™: {e}")

def test_multiprocess_gpu():
    """æµ‹è¯•å¤šè¿›ç¨‹GPUè®¿é—®"""
    print_section("å¤šè¿›ç¨‹GPUæµ‹è¯•")
    
    try:
        import multiprocessing
        
        def worker_gpu_test(worker_id):
            """Workerè¿›ç¨‹GPUæµ‹è¯•"""
            try:
                # è®¾ç½®GPUç¯å¢ƒå˜é‡
                os.environ['CUDA_VISIBLE_DEVICES'] = '0'
                
                # æµ‹è¯•ONNX Runtime
                import onnxruntime
                providers = onnxruntime.get_available_providers()
                onnx_cuda = 'CUDAExecutionProvider' in providers
                
                # æµ‹è¯•PyTorchï¼ˆå¦‚æœå¯ç”¨ï¼‰
                torch_cuda = False
                try:
                    import torch
                    torch_cuda = torch.cuda.is_available()
                except ImportError:
                    pass
                
                return f"Worker[{worker_id}]: ONNX={onnx_cuda}, PyTorch={torch_cuda}"
                
            except Exception as e:
                return f"Worker[{worker_id}]: Error - {e}"
        
        # å¯åŠ¨å¤šä¸ªè¿›ç¨‹æµ‹è¯•
        with multiprocessing.Pool(2) as pool:
            results = pool.map(worker_gpu_test, [1, 2])
            
        for result in results:
            print(f"ğŸ§ª {result}")
            
    except Exception as e:
        print(f"âŒ å¤šè¿›ç¨‹GPUæµ‹è¯•å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("HeyGem AI - GPU + Gunicorn ç¯å¢ƒè¯Šæ–­")
    print("=" * 60)
    
    check_basic_environment()
    check_nvidia_driver()
    check_onnx_runtime()
    check_pytorch()
    check_gunicorn_config()
    check_service_dependencies()
    test_multiprocess_gpu()
    
    print_section("è¯Šæ–­å®Œæˆ")
    print("ğŸ“‹ å»ºè®®æ£€æŸ¥é¡¹ç›®:")
    print("1. ç¡®ä¿ preload_app = False")
    print("2. å»ºè®® workers = 1 (é¿å…GPUæ˜¾å­˜ä¸è¶³)")
    print("3. ç¡®ä¿CUDAç¯å¢ƒå˜é‡æ­£ç¡®è®¾ç½®")
    print("4. æ¨¡å‹å¿…é¡»åœ¨workerè¿›ç¨‹ä¸­åˆå§‹åŒ–ï¼Œä¸èƒ½åœ¨ä¸»è¿›ç¨‹")
    print("\nå¦‚æœé—®é¢˜ä»ç„¶å­˜åœ¨ï¼Œè¯·æ£€æŸ¥æ—¥å¿—ä¸­çš„post_forkä¿¡æ¯")

if __name__ == "__main__":
    main() 